{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8b9fcc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Network study of the Bankruptcy Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e34eeb",
   "metadata": {},
   "source": [
    "Kudryavtsev O., Yazici M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a3d325e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3a59bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>inactive_active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.505452</td>\n",
       "      <td>0.037576</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>1.424141</td>\n",
       "      <td>0.750397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.523732</td>\n",
       "      <td>0.038025</td>\n",
       "      <td>0.072910</td>\n",
       "      <td>1.379251</td>\n",
       "      <td>0.849812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.449485</td>\n",
       "      <td>0.023832</td>\n",
       "      <td>0.021829</td>\n",
       "      <td>1.696345</td>\n",
       "      <td>0.981098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.370960</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.070648</td>\n",
       "      <td>1.444869</td>\n",
       "      <td>0.984495</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.391547</td>\n",
       "      <td>-0.158851</td>\n",
       "      <td>0.029688</td>\n",
       "      <td>0.722070</td>\n",
       "      <td>1.179211</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56410</th>\n",
       "      <td>0.582957</td>\n",
       "      <td>-0.019048</td>\n",
       "      <td>0.046115</td>\n",
       "      <td>0.215173</td>\n",
       "      <td>2.361905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56411</th>\n",
       "      <td>0.923624</td>\n",
       "      <td>-0.144440</td>\n",
       "      <td>-0.141252</td>\n",
       "      <td>11.325387</td>\n",
       "      <td>0.327095</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56412</th>\n",
       "      <td>0.778861</td>\n",
       "      <td>-0.101432</td>\n",
       "      <td>-0.081823</td>\n",
       "      <td>5.433620</td>\n",
       "      <td>0.631230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56413</th>\n",
       "      <td>0.597173</td>\n",
       "      <td>0.020475</td>\n",
       "      <td>0.054032</td>\n",
       "      <td>2.228618</td>\n",
       "      <td>0.774742</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56414</th>\n",
       "      <td>0.227763</td>\n",
       "      <td>0.024873</td>\n",
       "      <td>0.054223</td>\n",
       "      <td>0.281469</td>\n",
       "      <td>0.954408</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56415 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             X1        X2        X3         X4        X5  inactive_active\n",
       "0      0.505452  0.037576  0.056000   1.424141  0.750397                0\n",
       "1      0.523732  0.038025  0.072910   1.379251  0.849812                0\n",
       "2      0.449485  0.023832  0.021829   1.696345  0.981098                0\n",
       "3      0.370960  0.046000  0.070648   1.444869  0.984495                0\n",
       "4      0.391547 -0.158851  0.029688   0.722070  1.179211                0\n",
       "...         ...       ...       ...        ...       ...              ...\n",
       "56410  0.582957 -0.019048  0.046115   0.215173  2.361905                0\n",
       "56411  0.923624 -0.144440 -0.141252  11.325387  0.327095                1\n",
       "56412  0.778861 -0.101432 -0.081823   5.433620  0.631230                1\n",
       "56413  0.597173  0.020475  0.054032   2.228618  0.774742                1\n",
       "56414  0.227763  0.024873  0.054223   0.281469  0.954408                1\n",
       "\n",
       "[56415 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca4620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count and ratio of 1 (active firms) in tha data\n",
      "count:  44205 rate:  0.7835681999468227 total data : 56415\n"
     ]
    }
   ],
   "source": [
    "count_1 = data['inactive_active'].sum()\n",
    "count_0 = len(data['inactive_active']) - count_1\n",
    "print('The count and ratio of 1 (active firms) in tha data')\n",
    "print('count: ',count_1,'rate: ', count_1/(count_1+count_0), 'total data :', count_1+count_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33f1995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count and ratio of 0 (inactive firms) in tha data\n",
      "count:  12210 rate:  0.21643180005317736 total data : 56415\n"
     ]
    }
   ],
   "source": [
    "print('The count and ratio of 0 (inactive firms) in tha data')\n",
    "print('count: ',count_0,'rate: ', count_0/(count_1+count_0), 'total data :', count_1+count_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3f9c8",
   "metadata": {},
   "source": [
    "## PART 1: The Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034c7a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.886063269889062 1.1766834907332443e-09\n",
      "p-value: 1.1766834907332443e-09, The null hypothesis is rejected. There is a difference between at least two variables.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Convert data\n",
    "X = data.drop(['inactive_active'],axis=1)\n",
    "y = data['inactive_active']\n",
    "\n",
    "# The one-way ANOVA tests: \n",
    "# The null hypothesis that two or more groups have the same population mean.\n",
    "fvalue, pvalue = stats.f_oneway(X['X1'], X['X2'], X['X3'], X['X4'], X['X5'])\n",
    "print(fvalue, pvalue)\n",
    "\n",
    "if pvalue<0.05:\n",
    "    print(\n",
    "    \"p-value: {}, The null hypothesis is rejected. There is a difference between at least two variables.\".format(\n",
    "        pvalue))\n",
    "else:\n",
    "    print(\n",
    "    \"p-value: {}, The null hypothesis is accepted. There are not any differences among the means of variables.\".format(\n",
    "        pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "003395a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the input data is from a normal distribution\n",
      "the input data is from a normal distribution\n",
      "the input data is from a normal distribution\n",
      "the input data is from a normal distribution\n",
      "the input data is from a normal distribution\n"
     ]
    }
   ],
   "source": [
    "# // The Test of Normality //\n",
    "# The creating a function called normality()\n",
    "# The null hypothesis that the input data is not from a normal distribution.\n",
    "def normality(x):\n",
    "    k2, pvalue = stats.normaltest(x)\n",
    "    alpha = 1e-3\n",
    "    \n",
    "    if pvalue < alpha: # null hypothesis: x comes from a normal distribution\n",
    "        return('the input data is from a normal distribution')\n",
    "    else:\n",
    "        return('the input data is not from a normal distribution') \n",
    "    \n",
    "print(normality(X['X1']))\n",
    "print(normality(X['X2']))\n",
    "print(normality(X['X3']))\n",
    "print(normality(X['X4']))\n",
    "print(normality(X['X5']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d00b426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.261200887484931 0.0003103579423549121\n",
      "p-value: 1.1766834907332443e-09, The null hypothesis is rejected. Not all input samples are from populations with equal variances.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import levene\n",
    "\n",
    "# // Test of Homogeneity of Variances // \n",
    "# The leneve's test is used instead of Bartlettâ€™s test\n",
    "# because our data is from a normal distribution.\n",
    "# The null hypothesis that all input samples are from populations with equal variances.\n",
    "stat, p = levene(X['X1'], X['X2'], X['X3'], X['X4'], X['X5'])\n",
    "\n",
    "print(stat, p)\n",
    "\n",
    "if pvalue<0.05:\n",
    "    print(\n",
    "    \"p-value: {}, The null hypothesis is rejected. Not all input samples are from populations with equal variances.\".format(\n",
    "        pvalue))\n",
    "else:\n",
    "    print(\n",
    "    \"p-value: {}, The null hypothesis is accepted. All input samples are from populations with equal variances.\".format(\n",
    "        pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aef562b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X1</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.101612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.160139e-14</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X2</th>\n",
       "      <td>1.016125e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.565293</td>\n",
       "      <td>1.215299e-04</td>\n",
       "      <td>5.478299e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X3</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.565293</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X4</th>\n",
       "      <td>8.160139e-14</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.746425e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X5</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.054783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.746425e-11</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              X1        X2        X3            X4            X5\n",
       "X1  1.000000e+00  0.101612  0.000000  8.160139e-14  0.000000e+00\n",
       "X2  1.016125e-01  1.000000  0.565293  1.215299e-04  5.478299e-02\n",
       "X3  0.000000e+00  0.565293  1.000000  0.000000e+00  0.000000e+00\n",
       "X4  8.160139e-14  0.000122  0.000000  1.000000e+00  4.746425e-11\n",
       "X5  0.000000e+00  0.054783  0.000000  4.746425e-11  1.000000e+00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tamhaneâ€™s T2 all-pairs comparison test for normally distributed data with unequal variances. \n",
    "# Tamhaneâ€™s T2 test can be performed for all-pairs comparisons in an one-factorial layout with \n",
    "# normally distributed residuals but unequal groups variances. \n",
    "# A total of m = k(k-1)/2 hypotheses can be tested. \n",
    "# The null hypothesis is tested in the two-tailed test against the alternative hypothesis \n",
    "\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "x = pd.DataFrame({\"X1\": X['X1'], \"X2\": X['X2'], \"X3\": X['X3'], \"X4\": X['X4'],\"X5\": X['X5'], })\n",
    "x = x.melt(var_name='groups', value_name='values')\n",
    "sp.posthoc_tamhane(x, val_col='values', group_col='groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b5e7c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PART 2: The Neural Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44130842",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of inactive firms in the test data: 2401\n",
      "The count of active firms in the test data: 8882\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10000\n",
      "1411/1411 [==============================] - 50s 33ms/step - loss: 3.0736e-05 - tn: 5954.0000 - tp: 11914.0000 - val_loss: 0.6957 - val_tn: 1151.0000 - val_tp: 4132.0000\n",
      "Epoch 2/10000\n",
      "1411/1411 [==============================] - 46s 33ms/step - loss: 3.0736e-05 - tn: 5133.0000 - tp: 14489.0000 - val_loss: 0.6957 - val_tn: 1151.0000 - val_tp: 4132.0000\n",
      "Epoch 3/10000\n",
      "1411/1411 [==============================] - 44s 31ms/step - loss: 3.0736e-05 - tn: 5132.0000 - tp: 14490.0000 - val_loss: 0.6957 - val_tn: 1151.0000 - val_tp: 4132.0000\n",
      "Epoch 4/10000\n",
      "1411/1411 [==============================] - 41s 29ms/step - loss: 3.0736e-05 - tn: 5132.0000 - tp: 14489.0000 - val_loss: 0.6957 - val_tn: 1151.0000 - val_tp: 4132.0000\n",
      "Epoch 5/10000\n",
      "1411/1411 [==============================] - 48s 34ms/step - loss: 3.0736e-05 - tn: 5132.0000 - tp: 14492.0000 - val_loss: 0.6957 - val_tn: 1151.0000 - val_tp: 4132.0000\n",
      "Epoch 6/10000\n",
      "1411/1411 [==============================] - 48s 34ms/step - loss: 3.0736e-05 - tn: 5132.0000 - tp: 14493.0000 - val_loss: 0.6957 - val_tn: 1151.0000 - val_tp: 4132.0000\n",
      "Epoch 7/10000\n",
      "1411/1411 [==============================] - 46s 33ms/step - loss: 3.0736e-05 - tn: 5132.0000 - tp: 14494.0000 - val_loss: 0.6957 - val_tn: 1151.0000 - val_tp: 4133.0000\n",
      "Epoch 8/10000\n",
      "1411/1411 [==============================] - 49s 34ms/step - loss: 3.0736e-05 - tn: 5131.0000 - tp: 14494.0000 - val_loss: 0.6957 - val_tn: 1151.0000 - val_tp: 4133.0000\n",
      "Epoch 9/10000\n",
      "1411/1411 [==============================] - 47s 33ms/step - loss: 3.0736e-05 - tn: 5131.0000 - tp: 14495.0000 - val_loss: 0.6957 - val_tn: 1151.0000 - val_tp: 4133.0000\n",
      "Epoch 10/10000\n",
      "1411/1411 [==============================] - 47s 34ms/step - loss: 3.0736e-05 - tn: 5130.0000 - tp: 14495.0000 - val_loss: 0.6957 - val_tn: 1151.0000 - val_tp: 4133.0000\n",
      "Epoch 11/10000\n",
      "1411/1411 [==============================] - 48s 34ms/step - loss: 3.0736e-05 - tn: 5130.0000 - tp: 14495.0000 - val_loss: 0.6957 - val_tn: 1151.0000 - val_tp: 4133.0000\n",
      "Epoch 12/10000\n",
      "1411/1411 [==============================] - 46s 32ms/step - loss: 3.0736e-05 - tn: 5130.0000 - tp: 14495.0000 - val_loss: 0.6957 - val_tn: 1150.0000 - val_tp: 4133.0000\n",
      "Epoch 13/10000\n",
      "1411/1411 [==============================] - 48s 34ms/step - loss: 3.0736e-05 - tn: 5130.0000 - tp: 14495.0000 - val_loss: 0.6957 - val_tn: 1150.0000 - val_tp: 4133.0000\n",
      "Epoch 14/10000\n",
      "1411/1411 [==============================] - 47s 33ms/step - loss: 3.0736e-05 - tn: 5128.0000 - tp: 14499.0000 - val_loss: 0.6957 - val_tn: 1150.0000 - val_tp: 4133.0000\n",
      "Epoch 15/10000\n",
      "1411/1411 [==============================] - 47s 34ms/step - loss: 3.0736e-05 - tn: 5128.0000 - tp: 14499.0000 - val_loss: 0.6957 - val_tn: 1150.0000 - val_tp: 4133.0000\n",
      "Epoch 16/10000\n",
      "1411/1411 [==============================] - 48s 34ms/step - loss: 3.0736e-05 - tn: 5128.0000 - tp: 14499.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4133.0000\n",
      "Epoch 17/10000\n",
      "1411/1411 [==============================] - 47s 33ms/step - loss: 3.0736e-05 - tn: 5128.0000 - tp: 14499.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4133.0000\n",
      "Epoch 18/10000\n",
      "1411/1411 [==============================] - 47s 34ms/step - loss: 3.0736e-05 - tn: 5128.0000 - tp: 14500.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4133.0000\n",
      "Epoch 19/10000\n",
      "1411/1411 [==============================] - 47s 33ms/step - loss: 3.0736e-05 - tn: 5127.0000 - tp: 14501.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4134.0000\n",
      "Epoch 20/10000\n",
      "1411/1411 [==============================] - 48s 34ms/step - loss: 3.0736e-05 - tn: 5127.0000 - tp: 14504.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4134.0000\n",
      "Epoch 21/10000\n",
      "1411/1411 [==============================] - 49s 34ms/step - loss: 3.0736e-05 - tn: 5125.0000 - tp: 14505.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4134.0000\n",
      "Epoch 22/10000\n",
      "1411/1411 [==============================] - 47s 33ms/step - loss: 3.0736e-05 - tn: 5125.0000 - tp: 14507.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4134.0000\n",
      "Epoch 23/10000\n",
      "1411/1411 [==============================] - 48s 34ms/step - loss: 3.0736e-05 - tn: 5125.0000 - tp: 14506.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4135.0000\n",
      "Epoch 24/10000\n",
      "1411/1411 [==============================] - 46s 33ms/step - loss: 3.0736e-05 - tn: 5125.0000 - tp: 14507.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4135.0000\n",
      "Epoch 25/10000\n",
      "1411/1411 [==============================] - 48s 34ms/step - loss: 3.0736e-05 - tn: 5124.0000 - tp: 14508.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4136.0000\n",
      "Epoch 26/10000\n",
      "1411/1411 [==============================] - 47s 33ms/step - loss: 3.0736e-05 - tn: 5124.0000 - tp: 14510.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4136.0000\n",
      "Epoch 27/10000\n",
      "1411/1411 [==============================] - 47s 33ms/step - loss: 3.0736e-05 - tn: 5124.0000 - tp: 14510.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4137.0000\n",
      "Epoch 28/10000\n",
      "1411/1411 [==============================] - 48s 34ms/step - loss: 3.0736e-05 - tn: 5123.0000 - tp: 14511.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4137.0000\n",
      "Epoch 29/10000\n",
      "1411/1411 [==============================] - 44s 31ms/step - loss: 3.0736e-05 - tn: 5123.0000 - tp: 14511.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4137.0000\n",
      "Epoch 30/10000\n",
      "1411/1411 [==============================] - 49s 35ms/step - loss: 3.0736e-05 - tn: 5123.0000 - tp: 14511.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4137.0000\n",
      "Epoch 31/10000\n",
      "1411/1411 [==============================] - 49s 35ms/step - loss: 3.0736e-05 - tn: 5123.0000 - tp: 14513.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4137.0000\n",
      "Epoch 32/10000\n",
      "1411/1411 [==============================] - 42s 30ms/step - loss: 3.0736e-05 - tn: 5123.0000 - tp: 14513.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4137.0000\n",
      "Epoch 33/10000\n",
      "1411/1411 [==============================] - 42s 30ms/step - loss: 3.0736e-05 - tn: 5123.0000 - tp: 14514.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4138.0000\n",
      "Epoch 34/10000\n",
      "1411/1411 [==============================] - 43s 30ms/step - loss: 3.0736e-05 - tn: 5123.0000 - tp: 14516.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4139.0000\n",
      "Epoch 35/10000\n",
      "1411/1411 [==============================] - 40s 28ms/step - loss: 3.0736e-05 - tn: 5123.0000 - tp: 14517.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4139.0000\n",
      "Epoch 36/10000\n",
      "1411/1411 [==============================] - 40s 28ms/step - loss: 3.0736e-05 - tn: 5123.0000 - tp: 14517.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4139.0000\n",
      "Epoch 37/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0736e-05 - tn: 5122.0000 - tp: 14518.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4139.0000\n",
      "Epoch 38/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0736e-05 - tn: 5122.0000 - tp: 14519.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4139.0000\n",
      "Epoch 39/10000\n",
      "1411/1411 [==============================] - 40s 28ms/step - loss: 3.0736e-05 - tn: 5122.0000 - tp: 14520.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4139.0000\n",
      "Epoch 40/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0736e-05 - tn: 5122.0000 - tp: 14519.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4139.0000\n",
      "Epoch 41/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0736e-05 - tn: 5122.0000 - tp: 14521.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4139.0000\n",
      "Epoch 42/10000\n",
      "1411/1411 [==============================] - 40s 28ms/step - loss: 3.0736e-05 - tn: 5122.0000 - tp: 14521.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4139.0000\n",
      "Epoch 43/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0736e-05 - tn: 5122.0000 - tp: 14521.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4139.0000\n",
      "Epoch 44/10000\n",
      "1411/1411 [==============================] - 38s 27ms/step - loss: 3.0736e-05 - tn: 5120.0000 - tp: 14523.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4139.0000\n",
      "Epoch 45/10000\n",
      "1411/1411 [==============================] - 39s 27ms/step - loss: 3.0736e-05 - tn: 5121.0000 - tp: 14525.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4141.0000\n",
      "Epoch 46/10000\n",
      "1411/1411 [==============================] - 39s 27ms/step - loss: 3.0736e-05 - tn: 5120.0000 - tp: 14527.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4141.0000\n",
      "Epoch 47/10000\n",
      "1411/1411 [==============================] - 38s 27ms/step - loss: 3.0736e-05 - tn: 5120.0000 - tp: 14529.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4141.0000\n",
      "Epoch 48/10000\n",
      "1411/1411 [==============================] - 39s 27ms/step - loss: 3.0736e-05 - tn: 5120.0000 - tp: 14530.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4141.0000\n",
      "Epoch 49/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0736e-05 - tn: 5120.0000 - tp: 14531.0000 - val_loss: 0.6957 - val_tn: 1149.0000 - val_tp: 4141.0000\n",
      "Epoch 50/10000\n",
      "1411/1411 [==============================] - 39s 27ms/step - loss: 3.0736e-05 - tn: 5120.0000 - tp: 14531.0000 - val_loss: 0.6957 - val_tn: 1148.0000 - val_tp: 4142.0000\n",
      "Epoch 51/10000\n",
      "1411/1411 [==============================] - 39s 27ms/step - loss: 3.0736e-05 - tn: 5119.0000 - tp: 14533.0000 - val_loss: 0.6957 - val_tn: 1148.0000 - val_tp: 4143.0000\n",
      "Score for fold 1: loss of 0.6956763863563538; tn of 1151.0; tp of 4132.0%\n",
      "Score for fold 1: loss of 0.6958431005477905; tn of 5133.0; tp of 14489.0%\n",
      "a= 11283.0 b= 56414.0 c= 0.0 d= 11282.0\n",
      "The count of inactive firms in the test data: 1958\n",
      "The count of active firms in the test data: 9325\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0773e-05 - tn: 6040.0000 - tp: 16292.0000 - val_loss: 0.7211 - val_tn: 1137.0000 - val_tp: 4873.0000\n",
      "Epoch 2/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0773e-05 - tn: 6031.0000 - tp: 16346.0000 - val_loss: 0.7210 - val_tn: 1137.0000 - val_tp: 4873.0000\n",
      "Epoch 3/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0773e-05 - tn: 6030.0000 - tp: 16350.0000 - val_loss: 0.7210 - val_tn: 1136.0000 - val_tp: 4873.0000\n",
      "Epoch 4/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0773e-05 - tn: 6030.0000 - tp: 16353.0000 - val_loss: 0.7210 - val_tn: 1136.0000 - val_tp: 4873.0000\n",
      "Epoch 5/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0772e-05 - tn: 6030.0000 - tp: 16353.0000 - val_loss: 0.7210 - val_tn: 1136.0000 - val_tp: 4873.0000\n",
      "Epoch 6/10000\n",
      "1411/1411 [==============================] - 39s 27ms/step - loss: 3.0772e-05 - tn: 6030.0000 - tp: 16355.0000 - val_loss: 0.7210 - val_tn: 1136.0000 - val_tp: 4873.0000\n",
      "Epoch 7/10000\n",
      "1411/1411 [==============================] - 39s 27ms/step - loss: 3.0772e-05 - tn: 6030.0000 - tp: 16357.0000 - val_loss: 0.7210 - val_tn: 1135.0000 - val_tp: 4873.0000\n",
      "Epoch 8/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0772e-05 - tn: 6030.0000 - tp: 16359.0000 - val_loss: 0.7210 - val_tn: 1135.0000 - val_tp: 4873.0000\n",
      "Epoch 9/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0772e-05 - tn: 6029.0000 - tp: 16359.0000 - val_loss: 0.7210 - val_tn: 1135.0000 - val_tp: 4873.0000\n",
      "Epoch 10/10000\n",
      "1411/1411 [==============================] - 39s 27ms/step - loss: 3.0772e-05 - tn: 6029.0000 - tp: 16360.0000 - val_loss: 0.7210 - val_tn: 1135.0000 - val_tp: 4874.0000\n",
      "Epoch 11/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0772e-05 - tn: 6028.0000 - tp: 16362.0000 - val_loss: 0.7210 - val_tn: 1135.0000 - val_tp: 4874.0000\n",
      "Epoch 12/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0772e-05 - tn: 6028.0000 - tp: 16363.0000 - val_loss: 0.7210 - val_tn: 1135.0000 - val_tp: 4874.0000\n",
      "Epoch 13/10000\n",
      "1411/1411 [==============================] - 40s 28ms/step - loss: 3.0772e-05 - tn: 6028.0000 - tp: 16363.0000 - val_loss: 0.7209 - val_tn: 1134.0000 - val_tp: 4875.0000\n",
      "Epoch 14/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0772e-05 - tn: 6028.0000 - tp: 16364.0000 - val_loss: 0.7209 - val_tn: 1133.0000 - val_tp: 4876.0000\n",
      "Epoch 15/10000\n",
      "1411/1411 [==============================] - 40s 28ms/step - loss: 3.0772e-05 - tn: 6028.0000 - tp: 16367.0000 - val_loss: 0.7209 - val_tn: 1133.0000 - val_tp: 4877.0000\n",
      "Epoch 16/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0772e-05 - tn: 6028.0000 - tp: 16368.0000 - val_loss: 0.7209 - val_tn: 1132.0000 - val_tp: 4877.0000\n",
      "Epoch 17/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0772e-05 - tn: 6028.0000 - tp: 16370.0000 - val_loss: 0.7209 - val_tn: 1132.0000 - val_tp: 4877.0000\n",
      "Epoch 18/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0772e-05 - tn: 6028.0000 - tp: 16370.0000 - val_loss: 0.7209 - val_tn: 1132.0000 - val_tp: 4877.0000\n",
      "Epoch 19/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0772e-05 - tn: 6028.0000 - tp: 16371.0000 - val_loss: 0.7209 - val_tn: 1132.0000 - val_tp: 4877.0000\n",
      "Epoch 20/10000\n",
      "1411/1411 [==============================] - 40s 28ms/step - loss: 3.0772e-05 - tn: 6027.0000 - tp: 16372.0000 - val_loss: 0.7209 - val_tn: 1131.0000 - val_tp: 4877.0000\n",
      "Epoch 21/10000\n",
      "1411/1411 [==============================] - 40s 28ms/step - loss: 3.0772e-05 - tn: 6027.0000 - tp: 16374.0000 - val_loss: 0.7209 - val_tn: 1131.0000 - val_tp: 4877.0000\n",
      "Epoch 22/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0772e-05 - tn: 6027.0000 - tp: 16374.0000 - val_loss: 0.7209 - val_tn: 1131.0000 - val_tp: 4877.0000\n",
      "Epoch 23/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0772e-05 - tn: 6027.0000 - tp: 16375.0000 - val_loss: 0.7209 - val_tn: 1131.0000 - val_tp: 4878.0000\n",
      "Epoch 24/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0772e-05 - tn: 6026.0000 - tp: 16375.0000 - val_loss: 0.7209 - val_tn: 1131.0000 - val_tp: 4878.0000\n",
      "Epoch 25/10000\n",
      "1411/1411 [==============================] - 40s 28ms/step - loss: 3.0772e-05 - tn: 6025.0000 - tp: 16375.0000 - val_loss: 0.7208 - val_tn: 1131.0000 - val_tp: 4878.0000\n",
      "Epoch 26/10000\n",
      "1411/1411 [==============================] - 40s 29ms/step - loss: 3.0772e-05 - tn: 6023.0000 - tp: 16376.0000 - val_loss: 0.7208 - val_tn: 1131.0000 - val_tp: 4880.0000\n",
      "Epoch 27/10000\n",
      "1411/1411 [==============================] - 41s 29ms/step - loss: 3.0772e-05 - tn: 6022.0000 - tp: 16377.0000 - val_loss: 0.7208 - val_tn: 1131.0000 - val_tp: 4880.0000\n",
      "Epoch 28/10000\n",
      "1411/1411 [==============================] - 41s 29ms/step - loss: 3.0772e-05 - tn: 6022.0000 - tp: 16379.0000 - val_loss: 0.7208 - val_tn: 1131.0000 - val_tp: 4880.0000\n",
      "Epoch 29/10000\n",
      "1411/1411 [==============================] - 41s 29ms/step - loss: 3.0772e-05 - tn: 6021.0000 - tp: 16380.0000 - val_loss: 0.7208 - val_tn: 1131.0000 - val_tp: 4881.0000\n",
      "Epoch 30/10000\n",
      "1411/1411 [==============================] - 52s 37ms/step - loss: 3.0772e-05 - tn: 6021.0000 - tp: 16380.0000 - val_loss: 0.7208 - val_tn: 1131.0000 - val_tp: 4882.0000\n",
      "Epoch 31/10000\n",
      "1411/1411 [==============================] - 45s 32ms/step - loss: 3.0772e-05 - tn: 6021.0000 - tp: 16380.0000 - val_loss: 0.7208 - val_tn: 1131.0000 - val_tp: 4883.0000\n",
      "Epoch 32/10000\n",
      "1411/1411 [==============================] - 54s 38ms/step - loss: 3.0772e-05 - tn: 6021.0000 - tp: 16380.0000 - val_loss: 0.7208 - val_tn: 1131.0000 - val_tp: 4884.0000\n",
      "Epoch 33/10000\n",
      "1411/1411 [==============================] - 44s 32ms/step - loss: 3.0771e-05 - tn: 6021.0000 - tp: 16382.0000 - val_loss: 0.7208 - val_tn: 1131.0000 - val_tp: 4884.0000\n",
      "Epoch 34/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0771e-05 - tn: 6021.0000 - tp: 16382.0000 - val_loss: 0.7208 - val_tn: 1131.0000 - val_tp: 4885.0000\n",
      "Epoch 35/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0771e-05 - tn: 6021.0000 - tp: 16383.0000 - val_loss: 0.7208 - val_tn: 1131.0000 - val_tp: 4885.0000\n",
      "Epoch 36/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0771e-05 - tn: 6021.0000 - tp: 16384.0000 - val_loss: 0.7208 - val_tn: 1131.0000 - val_tp: 4885.0000\n",
      "Epoch 37/10000\n",
      "1411/1411 [==============================] - 39s 27ms/step - loss: 3.0771e-05 - tn: 6021.0000 - tp: 16386.0000 - val_loss: 0.7207 - val_tn: 1131.0000 - val_tp: 4885.0000\n",
      "Epoch 38/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0771e-05 - tn: 6021.0000 - tp: 16386.0000 - val_loss: 0.7207 - val_tn: 1131.0000 - val_tp: 4886.0000\n",
      "Epoch 39/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0771e-05 - tn: 6021.0000 - tp: 16386.0000 - val_loss: 0.7207 - val_tn: 1131.0000 - val_tp: 4886.0000\n",
      "Epoch 40/10000\n",
      "1411/1411 [==============================] - 39s 27ms/step - loss: 3.0771e-05 - tn: 6021.0000 - tp: 16388.0000 - val_loss: 0.7207 - val_tn: 1131.0000 - val_tp: 4887.0000\n",
      "Epoch 41/10000\n",
      "1411/1411 [==============================] - 40s 28ms/step - loss: 3.0771e-05 - tn: 6021.0000 - tp: 16389.0000 - val_loss: 0.7207 - val_tn: 1130.0000 - val_tp: 4887.0000\n",
      "Epoch 42/10000\n",
      "1411/1411 [==============================] - 40s 28ms/step - loss: 3.0771e-05 - tn: 6021.0000 - tp: 16390.0000 - val_loss: 0.7207 - val_tn: 1130.0000 - val_tp: 4888.0000\n",
      "Epoch 43/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0771e-05 - tn: 6020.0000 - tp: 16391.0000 - val_loss: 0.7207 - val_tn: 1130.0000 - val_tp: 4888.0000\n",
      "Epoch 44/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0771e-05 - tn: 6020.0000 - tp: 16392.0000 - val_loss: 0.7207 - val_tn: 1130.0000 - val_tp: 4888.0000\n",
      "Epoch 45/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0771e-05 - tn: 6020.0000 - tp: 16392.0000 - val_loss: 0.7207 - val_tn: 1130.0000 - val_tp: 4888.0000\n",
      "Epoch 46/10000\n",
      "1411/1411 [==============================] - 39s 27ms/step - loss: 3.0771e-05 - tn: 6020.0000 - tp: 16392.0000 - val_loss: 0.7207 - val_tn: 1130.0000 - val_tp: 4888.0000\n",
      "Epoch 47/10000\n",
      "1411/1411 [==============================] - 39s 27ms/step - loss: 3.0771e-05 - tn: 6019.0000 - tp: 16392.0000 - val_loss: 0.7207 - val_tn: 1130.0000 - val_tp: 4889.0000\n",
      "Epoch 48/10000\n",
      "1411/1411 [==============================] - 40s 28ms/step - loss: 3.0771e-05 - tn: 6019.0000 - tp: 16393.0000 - val_loss: 0.7207 - val_tn: 1130.0000 - val_tp: 4889.0000\n",
      "Epoch 49/10000\n",
      "1411/1411 [==============================] - 39s 27ms/step - loss: 3.0771e-05 - tn: 6018.0000 - tp: 16393.0000 - val_loss: 0.7206 - val_tn: 1130.0000 - val_tp: 4890.0000\n",
      "Epoch 50/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0771e-05 - tn: 6017.0000 - tp: 16394.0000 - val_loss: 0.7206 - val_tn: 1130.0000 - val_tp: 4890.0000\n",
      "Epoch 51/10000\n",
      "1411/1411 [==============================] - 39s 28ms/step - loss: 3.0771e-05 - tn: 6016.0000 - tp: 16395.0000 - val_loss: 0.7206 - val_tn: 1129.0000 - val_tp: 4890.0000\n",
      "Score for fold 2: loss of 0.7210500836372375; tn of 1137.0; tp of 4873.0%\n",
      "Score for fold 2: loss of 0.7266331315040588; tn of 6031.0; tp of 16346.0%\n",
      "a= 22566.0 b= 11282.0 c= 11283.0 d= 22565.0\n",
      "The count of inactive firms in the test data: 2172\n",
      "The count of active firms in the test data: 9111\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8fdf208bd379>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# fit the keras model on the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         model[i].fit(X_train, y_train, epochs=10000, \n\u001b[0m\u001b[0;32m     87\u001b[0m                    \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                       class_weight=class_weight)\n",
      "\u001b[1;32m~\\anaconda_3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1393\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1395\u001b[1;33m           raise ValueError('Unexpected result of `train_function` '\n\u001b[0m\u001b[0;32m   1396\u001b[0m                            \u001b[1;34m'(Empty logs). Please use '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m                            \u001b[1;34m'`Model.compile(..., run_eagerly=True)`, or '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#initial values for the index of splitted data\n",
    "# n = X.shape[0] = 56415\n",
    "# The train and test data parts of the data are as follows\n",
    "# Train: X.loc[a:b] and Test: X.loc[c:d] \n",
    "# The lenght of the train data is 4 times of the test data.\n",
    "n=56415\n",
    "a=0; b=(4*n/5)-1; c=4*n/5 ; d=n-1\n",
    "\n",
    "fold_no = 1\n",
    "loss_per_fold, fn_per_fold, fp_per_fold = [], [], []\n",
    "tn_per_fold, tp_per_fold, pr_per_fold, roc_per_fold = [], [], [], []\n",
    "\n",
    "model= [0,1,2,3,4]; history= [0,1,2,3,4]\n",
    "for i in range(5):\n",
    "    for j in range(1,6):\n",
    "        X_train, X_test, y_train, y_test = [], [], [], []\n",
    "        X_train, X_test = X.loc[a:b], X.loc[c:d]\n",
    "        y_train, y_test = y.loc[a:b], y.loc[c:d]\n",
    "        \n",
    "        count_tn_test, count_tp_test = 0, 0\n",
    "        for i in y_test:\n",
    "            if i==0:\n",
    "                count_tn_test+=1\n",
    "                \n",
    "            if i==1:\n",
    "                count_tp_test+=1\n",
    "        \n",
    "        print('The count of inactive firms in the test data:', count_tn_test)\n",
    "        print('The count of active firms in the test data:', count_tp_test)\n",
    "\n",
    "        # Analyze class imbalance in the targets\n",
    "        # 0 and 1 mean inactive, active firms respectively.\n",
    "        counts_1 = y_train.sum()\n",
    "        counts_0 = len(y_train) - counts_1\n",
    "\n",
    "        # The weighting for the imlanabce\n",
    "        weight_for_0 = 1.0 / counts_0\n",
    "        weight_for_1 = 1.0 / counts_1\n",
    "\n",
    "        # Normalize the data using training set statistics\n",
    "        mean = np.mean(X_train, axis=0)\n",
    "        X_train -= mean\n",
    "        X_test -= mean\n",
    "        std = np.std(X_train, axis=0)\n",
    "        X_train /= std\n",
    "        X_test /= std\n",
    "\n",
    "        # Build a binary classification model\n",
    "        model[i] = Sequential()\n",
    "        model[i].add(keras.Input(shape=(5,)))\n",
    "        model[i].add(Dense(1, use_bias=True, activation=\"relu\")) \n",
    "        #model[i].add(Dense(3, use_bias=True, activation=\"relu\")) \n",
    "        model[i].add(Dense(1, activation=\"sigmoid\")) \n",
    "        \n",
    "\n",
    "        # define the keras model\n",
    "        metrics = [\n",
    "            keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "            keras.metrics.TruePositives(name=\"tp\"),\n",
    "            #keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "            #keras.metrics.FalsePositives(name=\"fp\"),\n",
    "            #keras.metrics.Precision(name=\"pr\"),\n",
    "            #keras.metrics.Recall(name=\"rc\"),\n",
    "            #keras.metrics.AUC(name=\"auc\")\n",
    "        ]\n",
    "\n",
    "        # compile the keras model\n",
    "        model[i].compile(\n",
    "            optimizer=keras.optimizers.SGD(), \n",
    "            loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "            metrics=metrics, run_eagerly=True\n",
    "        )\n",
    "\n",
    "        callbacks = [keras.callbacks.EarlyStopping(monitor='val_tn', mode='max', patience=50, restore_best_weights=True)]\n",
    "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "        # Generate a print\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "        # fit the keras model on the dataset\n",
    "        model[i].fit(X_train, y_train, epochs=10000, \n",
    "                   callbacks=[callbacks],validation_data=(X_test, y_test),\n",
    "                      class_weight=class_weight)\n",
    "        \n",
    "        # Generate generalization metrics\n",
    "        scores_test = model[i].evaluate(X_test, y_test, verbose=0)\n",
    "        scores_train = model[i].evaluate(X_train, y_train, verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {model[i].metrics_names[0]} of {scores_test[0]}; {model[i].metrics_names[1]} of {scores_test[1]}; {model[i].metrics_names[2]} of {scores_test[2]}%')\n",
    "        print(f'Score for fold {fold_no}: {model[i].metrics_names[0]} of {scores_train[0]}; {model[i].metrics_names[1]} of {scores_train[1]}; {model[i].metrics_names[2]} of {scores_train[2]}%')\n",
    "\n",
    "        \n",
    "        loss_per_fold.append(scores_test[0])\n",
    "        tn_per_fold.append(scores_test[1])\n",
    "        tp_per_fold.append(scores_test[2])\n",
    "        loss_per_fold.append(scores_train[0])\n",
    "        tn_per_fold.append(scores_train[1])\n",
    "        tp_per_fold.append(scores_train[2])\n",
    "        #auc_per_fold.append(scores[3])\n",
    "        #fn_per_fold.append(scores[3])\n",
    "        #fp_per_fold.append(scores[4])\n",
    "        #pr_per_fold.append(scores[5])\n",
    "        #roc_per_fold.append(scores[6])\n",
    "\n",
    "        # Updating on border of parts of the data\n",
    "\n",
    "        a+= n/5; b+= n/5; c+= n/5; d+= n/5 \n",
    "\n",
    "        if a>=n:\n",
    "            a=a-n\n",
    "        if b>=n:\n",
    "            b=b-n\n",
    "        if c>=n:\n",
    "            c=c-n\n",
    "        if d>=n:\n",
    "            d=d-n\n",
    "        print('a=', a, 'b=',b,'c=',c,'d=',d)    \n",
    "        \n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5abc696",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[-0.49870253, -0.34028733, -0.27374184],\n",
       "         [ 0.79215086, -0.431607  , -0.09532931],\n",
       "         [ 1.4825865 ,  0.5765368 ,  0.24960464],\n",
       "         [-0.4809401 ,  0.07675144, -2.1606798 ],\n",
       "         [-0.46204314,  0.68306607,  0.5899975 ]], dtype=float32),\n",
       "  array([-0.05728413, -1.0670689 , -0.27597493], dtype=float32)],\n",
       " [array([[ 0.5370146],\n",
       "         [ 0.3890563],\n",
       "         [-0.2800261]], dtype=float32),\n",
       "  array([-0.16184498], dtype=float32)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = []\n",
    "for layer in model[0].layers:\n",
    "    weights.append(layer.get_weights())\n",
    "    \n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dddfaddc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[-0.50863326,  0.05512453, -0.07973937],\n",
       "         [-0.60945797, -0.63594097, -0.5271471 ],\n",
       "         [ 1.5030128 ,  0.45033693,  0.29434103],\n",
       "         [-0.08043813, -0.40192685, -0.32424206],\n",
       "         [-0.46885785, -0.05854674,  0.15191257]], dtype=float32),\n",
       "  array([-0.02235349, -0.12361064, -0.7375796 ], dtype=float32)],\n",
       " [array([[ 0.46709156],\n",
       "         [-0.00100073],\n",
       "         [ 0.04784242]], dtype=float32),\n",
       "  array([-0.16520314], dtype=float32)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = []\n",
    "for layer in model[1].layers:\n",
    "    weights.append(layer.get_weights())\n",
    "    \n",
    "weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c17c9e3",
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[-3.6298841e-02, -3.1758866e-01, -7.2755534e-03],\n",
       "         [-1.8320817e-01, -1.9358918e-01,  1.5903609e-03],\n",
       "         [ 2.5006363e-01,  1.6751556e+00,  6.7165536e-01],\n",
       "         [-4.0036520e-01, -3.1600782e-01, -1.1875194e-02],\n",
       "         [-5.2829665e-01, -3.9048678e-01, -7.6732837e-02]], dtype=float32),\n",
       "  array([-0.45049477, -0.00852251, -0.15321407], dtype=float32)],\n",
       " [array([[ 0.17760336],\n",
       "         [ 0.5252656 ],\n",
       "         [-0.67364   ]], dtype=float32),\n",
       "  array([-0.13926245], dtype=float32)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = []\n",
    "for layer in model[2].layers:\n",
    "    weights.append(layer.get_weights())\n",
    "    \n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef3f4915",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.1922297 , -0.65864605, -0.03421036],\n",
       "         [-0.12225977,  1.1038945 ,  0.53465796],\n",
       "         [ 0.16285181, -0.3169435 , -0.1571946 ],\n",
       "         [-0.22447477, -0.24603991, -0.7175949 ],\n",
       "         [-0.9024451 , -0.88144696, -0.6796323 ]], dtype=float32),\n",
       "  array([-0.5493661 ,  0.12501346, -0.37942746], dtype=float32)],\n",
       " [array([[ 0.003272  ],\n",
       "         [ 0.3544913 ],\n",
       "         [-0.25608635]], dtype=float32),\n",
       "  array([-0.22150855], dtype=float32)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = []\n",
    "for layer in model[3].layers:\n",
    "    weights.append(layer.get_weights())\n",
    "    \n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a46108e8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.4809681 , -0.02593469, -0.07389922],\n",
       "         [ 0.08352622,  1.2002393 ,  0.23962002],\n",
       "         [ 0.15392269,  0.00218992,  0.42190522],\n",
       "         [-0.04256022,  0.48634708,  0.49245352],\n",
       "         [-0.00445289, -0.12019757,  0.03835237]], dtype=float32),\n",
       "  array([ 0.67079276, -0.26240855, -0.8451711 ], dtype=float32)],\n",
       " [array([[-0.29074916],\n",
       "         [-0.38122767],\n",
       "         [ 0.78662133]], dtype=float32),\n",
       "  array([0.18905252], dtype=float32)]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = []\n",
    "for layer in model[4].layers:\n",
    "    weights.append(layer.get_weights())\n",
    "    \n",
    "weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
